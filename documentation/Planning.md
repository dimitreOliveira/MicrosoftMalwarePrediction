# Microsoft Malware Prediction - Planning

### Working effort:
 1. Literature review (read some kernels(at least 5) and relevant content related to malware prediction);
 2. Data exploration (replicate EDA done by others and try to find some new insights);
 3. Algorithm development (algorithms that seems to be promissing);
 4. Result analysis (review model performances, competition metric is AUC (area under the ROC curve));
 5. Review (review each model process including it's data generation).
 
### Working cycle:
1. Read competition info, existing info and relevant content to feel confortable with the problem. Create hyphotesis based in that info;
2. Initial data exploration, again to feel confortable with the problem and the data;
3. Build the first implementation (simple);
4. Loop throught [Analyze -> Approach(model) -> Implement -> Measure].

### 1. Literature review (read some kernels(at least 5) and relevant content related to malware prediction) (post links bellow).
* Kernels:
    * [Malware Detection - EDA and LGBM](https://www.kaggle.com/dimitreoliveira/malware-detection-eda-and-lgbm) [Dimitre]
    * [Microsoft Malware Prediction - EDA with Tableau](https://www.kaggle.com/tatianass/microsoft-malware-prediction-eda-with-tableau) [Tatiana]
    * [Detecting Malwares with LGBM](https://www.kaggle.com/fabiendaniel/detecting-malwares-with-lgbm)
    * [My EDA - I want to see all! by YouHan Lee](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)
    * [Is this Malware? [EDA, FE and lgb][updated]](https://www.kaggle.com/artgor/is-this-malware-eda-fe-and-lgb-updated)
 
* Relevant content:
    * https://en.wikipedia.org/wiki/Malware
     
* Insights:
    * Model training and evaluation should be done with at least 5 million rows, otherwise some features may be inconsistent.
    * Train and test sets have a temporal variation, test begins at the same time than train but ends 2 months later, train and validation split should follow the same pattern.
    * Train and test sets have 5 years of data, but most of it is accumulated at the last 3 months.
    * The train and test set all have unique IDs.
    * Train have mostly categorical data.
    * Some features have far too much missing data to be used: (PuaMode, Census_ProcessorClass, DefaultBrowsersIdentifier, Census_InternalBatteryType, Census_IsFlightingInternal, Census_ThresholdOptIn, Census_IsWIMBootEnabled, SmartScreen, OrganizationIdentifier)
    * Some features have large cardinality, and should not be used without some pre process: (AvSigVersion, DefaultBrowsersIdentifier, AVProductStatesIdentifier, CityIdentifier, Census_OEMNameIdentifier, Census_OEMModelIdentifier, Census_ProcessorModelIdentifier, Census_InternalBatteryNumberOfCharger, Census_PrimaryDiskTotalCapacity, Census_TotalPhysicalRAM, Census_InternalPrimaryDisplayResolutionHorizontal, Census_FirmwareVersionIdentifier, Census_SystemVolumeTotalCapacity)
    * Version or Build features that have the same label (or similar) distribution: (EngineVersion, AppVersion, OsBuild, Census_OSBuildNumber)
    * Numerical features that have the same label (or similar) distribution: (Census_PrimaryDiskTotalCapacity, Census_InternalPrimaryDisplayResolutionHorizontal, Census_InternalBatteryNumberOfCharges)
    * **DefaultBrowsersIdentifier, Census_IsFlightingInternal, Census_InternalBatteryType** are not meaningful;