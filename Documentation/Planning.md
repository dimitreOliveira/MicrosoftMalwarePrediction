# Microsoft Malware Prediction - Planning

## Working effort:
 1. Literature review (read some kernels(at least 5) and relevant content related to malware prediction);
 2. Data exploration (replicate EDA done by others and try to find some new insights);
 3. Algorithm development (algorithms that seems to be promissing);
 4. Result analysis (review model performances, competition metric is AUC (area under the ROC curve));
 5. Review (review each model process including it's data generation).
 
## Working cycle:
1. Read competition info, existing info and relevant content to feel confortable with the problem. Create hyphotesis based in that info;
2. Initial data exploration, again to feel confortable with the problem and the data;
3. Build the first implementation (simple);
4. Loop throught [Analyze -> Approach(model) -> Implement -> Measure].

## 1. Literature review (read some kernels(at least 5) and relevant content related to malware prediction) (post links bellow).
* ### Kernels:
    * [Malware Detection - EDA and LGBM](https://www.kaggle.com/dimitreoliveira/malware-detection-eda-and-lgbm) [Dimitre]
    * [Microsoft Malware Prediction - EDA with Tableau](https://www.kaggle.com/tatianass/microsoft-malware-prediction-eda-with-tableau) [Tatiana]
    * [Detecting Malwares with LGBM](https://www.kaggle.com/fabiendaniel/detecting-malwares-with-lgbm)
    * [My EDA - I want to see all! by YouHan Lee](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)
    * [Is this Malware? [EDA, FE and lgb][updated]](https://www.kaggle.com/artgor/is-this-malware-eda-fe-and-lgb-updated)
    * [Malware Prediction - Adversarial Validation](https://www.kaggle.com/tatianass/malware-prediction-adversarial-validation)
 
* ### Relevant content:
    * https://en.wikipedia.org/wiki/Malware
     
* ### Insights:
    * Model training and evaluation should be done with at least 5 million rows, otherwise some features may be inconsistent.
    * Train and test sets have a temporal variation, test begins at the same time than train but ends 2 months later, train and validation split should follow the same pattern.
    * Train and test sets have 5 years of data, but most of it is accumulated at the last 3 months.
    * The train and test set all have unique IDs.
    * Train have mostly categorical data.
    * The train data should be wisely chosen to represent the test data, otherwise the results when training arent't as good.
    * Localization related features are important to the result.
 * #### Positive features
  * **AVProductsInstalled, Census_ProcessorCoreCount** seems to be useful, they have different label distribution.
 * #### Negative features
    * Some features have far too much missing data to be used: **(PuaMode, Census_ProcessorClass, DefaultBrowsersIdentifier, Census_InternalBatteryType, Census_IsFlightingInternal, Census_ThresholdOptIn, Census_IsWIMBootEnabled, SmartScreen, OrganizationIdentifier)**.
    * Some features have large cardinality, and should not be used without some pre process: **(AvSigVersion, DefaultBrowsersIdentifier, AVProductStatesIdentifier, CityIdentifier, Census_OEMNameIdentifier, Census_OEMModelIdentifier, Census_ProcessorModelIdentifier, Census_InternalBatteryNumberOfCharger, Census_PrimaryDiskTotalCapacity, Census_TotalPhysicalRAM, Census_InternalPrimaryDisplayResolutionHorizontal, Census_FirmwareVersionIdentifier, Census_SystemVolumeTotalCapacity)**.
    * Version or Build features that have the same (or similar) label distribution: **(EngineVersion, AppVersion, OsBuild, Census_OSBuildNumber)**.
    * Numerical features that have the same (or similar) label distribution: **(Census_PrimaryDiskTotalCapacity, Census_InternalPrimaryDisplayResolutionHorizontal, Census_InternalBatteryNumberOfCharges)**.
    * Binary features that have almost the same distribution on the label: **(IsBeta, HasTpm, AutoSampleOptIn, SMode, Firewall, Census_HasOpticalDiskDrive, Census_IsPortableOperatingSystem, Census_IsFlightsDisabled, Census_IsSecureBootEnabled, Census_IsPenCapable)**.
    * Some categorical features have a 10% increase or decrease in the number of categories between train and test sets, these features may need special treatment: **(AVProductsInstalled, OsVer, Census_InternalBatteryType, AVProductStatesIdentifier, MachineIdentifier, Census_InternalBatteryNumberOfCharges, DefaultBrowsersIdentifier, Census_FlightRing, RtpStateBitfield)**.
    * **DefaultBrowsersIdentifier, Census_IsFlightingInternal, Census_InternalBatteryType** are not meaningful.
    * Some features have 80% of the data concentrated in only one category: **AutoSampleOptln, AVProductsEnabled, Census_DeviceFamily, AVProductStatesIdentifier, Census_FlightRing, Census_GenuineStateName, Census_HasOpticalDiskDrive, Census_IsAlwaysOnAlwaysConnectedCapable, Census_IsFlightingInternal, Census_IsFlightsDisabled, Census_IsPenCapable, Census_IsPortableOperatingSystem, Census_IsTouchEnabled, Census_OSArchitecture, Census_ProcessorManufacturerIdentifier, Firewall, HasTpm, IsBeta, IsProtected, IsSxsPassiveMode, OsVer, Platform, Processor, ProductName, RtpStateBitfield, SMode**.
    * Some features of common sense importance are not relevant to the model, such as **Firmware and EngineVersion**.
    * For features with low cardinality:
        * Even distribution of malwares: Features that have in average the same malware frequecy for each category (spikes and hard falls excluded). Those are: Census_OSInstallTypeName, Wdft_RegionIdentifier, OsPlatformSubRelease, Census_PrimaryDiskTypeName, 
        * Not even distribution: Have spikes and hard falls for some categories or many: Census_PowerPlatformRoleName, Census_OSSkuName, ProductName, Census_InternalBatteryType, Census_DeviceFamily, Census_OSBranch, Census_ActivationChannel, SmartScreen, Census_ProcessorManufacturerIdentifier, UacLuaenable, Census_OSInstallLanguageIdentifier, Census_OSArchitecture, Census_OSEdition, SkuEdition, Census_OSWUAutoUpdateOptionsName, Census_MDC2FormFactor, OsSuite, Census_ChassisTypeName, Census_GenuineStateName, OrganizationIdentifier, Census_FlightRing.
        * Just Census_ChassisTypeName has proportion of missing values that are not between 40-60%.
    * For features with high cardinality:
        * Two features have a missing percentage not between 40-50%: AVProductStatesIdentifier and GeoNameIdentifier.
